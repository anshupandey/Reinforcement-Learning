{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M12S4_Solution_Reinforcement_Taxi_v3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshupandey/Reinforcement-Learning/blob/main/Code_RL_Deep_Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning"
      ],
      "metadata": {
        "id": "qm1mAJJFAUkl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Q Learning"
      ],
      "metadata": {
        "id": "tin0qyFwAUsH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='#4073FF'>Taxi v3 Reinforcement Learning</font>"
      ],
      "metadata": {
        "id": "2ll6GPZHAUyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='#14AAF5'>Implement Taxi-v3 game in Reinforcement Learning and try to make the agent more smarter by adding more complexity.</font>  "
      ],
      "metadata": {
        "id": "mShCpqB2AU5n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "###The Taxi Problem \n",
        "There are four designated locations in the grid world indicated by R(ed),\n",
        "G(reen), Y(ellow), and B(lue). When the episode starts, the taxi starts off\n",
        "at a random square and the passenger is at a random location. The taxi\n",
        "drives to the passenger's location, picks up the passenger, drives to the\n",
        "passenger's destination (another one of the four specified locations), and\n",
        "then drops off the passenger. Once the passenger is dropped off, the episode ends.\n",
        "\n",
        "Map:\n",
        "\n",
        "![Screenshot from 2022-03-03 10-12-38.png](data:image/png;base64,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)\n",
        "\n",
        "### Actions\n",
        "There are 6 discrete deterministic actions:\n",
        "  - 0: move south\n",
        "  - 1: move north\n",
        "  - 2: move east\n",
        "  - 3: move west\n",
        "  - 4: pickup passenger\n",
        "  - 5: drop off passenger\n",
        "\n",
        "### Observations\n",
        "There are 500 discrete states since there are 25 taxi positions, 5 possible\n",
        "locations of the passenger (including the case when the passenger is in the taxi), and 4 destination locations.\n",
        "\n",
        "Note that there are 400 states that can actually be reached during an\n",
        "episode. The missing states correspond to situations in which the passenger\n",
        "is at the same location as their destination, as this typically signals the\n",
        "end of an episode. Four additional states can be observed right after a\n",
        "successful episodes, when both the passenger and the taxi are at the destination.\n",
        "\n",
        "This gives a total of 404 reachable discrete states.\n",
        "Passenger locations:\n",
        "  - 0: R(ed)\n",
        "  - 1: G(reen)\n",
        "  - 2: Y(ellow)\n",
        "  - 3: B(lue)\n",
        "  - 4: in taxi\n",
        "\n",
        "Destinations:\n",
        "  - 0: R(ed)\n",
        "  - 1: G(reen)\n",
        "  - 2: Y(ellow)\n",
        "  - 3: B(lue)\n",
        "\n",
        "Rewards\n",
        "  - -1 per step unless other reward is triggered.\n",
        "  - +20 delivering passenger.\n",
        "  - -10  executing \"pickup\" and \"drop-off\" actions illegally.\n",
        "  \n",
        "Rendering\n",
        "  - blue: passenger\n",
        "  - magenta: destination\n",
        "  - yellow: empty taxi\n",
        "  - green: full taxi\n",
        "  - other letters (R, G, Y and B): locations for passengers and destinations\n",
        "state space is represented by:\n",
        "  -(taxi_row, taxi_col, passenger_location, destination)"
      ],
      "metadata": {
        "id": "FrRLuoSNM0mI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtKdQ295_1Xy"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "import numpy as np\n",
        "import gym\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Initializing the environment\n",
        "\n",
        "Here we want taxi-v3. OpenAI Gym is a library composed of environments that we can use to train our agents. \n",
        "\n",
        "You can explore more about environments at: https://gym.openai.com/envs/#classic_control"
      ],
      "metadata": {
        "id": "yoJ2x4j4C1i2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an environment\n",
        "env = gym.make(\"Taxi-v3\")"
      ],
      "metadata": {
        "id": "2CgbWbzPCeyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rendering\n",
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbaKM0oPClPQ",
        "outputId": "fc47cf9c-bbcf-45b1-ca4f-ed20f07126e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | :\u001b[43m \u001b[0m| : |\n",
            "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now make a Q-table but to know the size of Q-table we first need to know the number of states and actions our environment has. \n",
        "\n",
        "To know this we can use the *`env.action_space.n`* and *`env.observation_space.n`*"
      ],
      "metadata": {
        "id": "vOhJk95-EA1V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Number of states and actions"
      ],
      "metadata": {
        "id": "D2LijhbOw8Z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "action_size = env.action_space.n\n",
        "print(\"Action-Size: \", action_size)\n",
        "\n",
        "state_size = env.observation_space.n\n",
        "print(\"State-Size: \", state_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chBM6aC8CsmI",
        "outputId": "a47af5b7-a8fb-4577-f22e-8dec71655d7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action-Size:  6\n",
            "State-Size:  500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Defining a Q-table"
      ],
      "metadata": {
        "id": "dNuMxrKCw4jB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a q table\n",
        "q_table = np.zeros((state_size, action_size))"
      ],
      "metadata": {
        "id": "nzbyubBfE9Zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_table"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-feQEz0FE5J",
        "outputId": "38c461ef-736f-4753-d554-719bea27c3b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "In Deep Q-Learning, the input to the neural network are possible states of the environment and the output of the neural network is the action to be taken. \n",
        "\n",
        "The input_length for a discrete environment in OpenAi's gym (e.g Taxi, Frozen Lake) is 1 because the output from env.step(env.action_space.sample())[0] (e.g. the state it will be in), is a single number."
      ],
      "metadata": {
        "id": "kUjm4yyRFc_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Embedding, Reshape\n",
        "from tensorflow.keras import optimizers"
      ],
      "metadata": {
        "id": "5EOyZ90eFMKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()\n",
        "env.step(env.action_space.sample())[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAnEgKxSQGYe",
        "outputId": "75f7a11f-e6ec-4e83-d518-ae10da01c8b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "348"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In the Embedding layer, the input_dimensions refers to the number of states and output_dimensions refers to the vector space we are squishing it to. This means that we have 500 possible states and we want it to be represented by 6 values.\n",
        "\n",
        "If you do not want to add any dense layers (meaning that you only want a single layer neural network, which is the embedding layer), you will have to set the output_dimensions of the Embedding layer to be the same as the action space of the environment. This means that output_dimensions must be 6 when you are using the Taxi environment because there can only be 6 actions, which are go up, go down, go left, go right, pickup passenger and drop passenger.\n"
      ],
      "metadata": {
        "id": "rOT5NAu4UCkS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. Modelling"
      ],
      "metadata": {
        "id": "L9kc1fpAwynt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-rl2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CICgwZaQr9y",
        "outputId": "b4a1120c-246c-4d36-ba40-9a6303cefc75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-rl2\n",
            "  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████▎                         | 10 kB 20.1 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 20 kB 25.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 30 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 40 kB 33.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 51 kB 36.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 52 kB 689 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.0.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.24.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (13.0.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 55.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.5.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.21.5)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.13.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.44.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (57.4.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.10.0.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras-rl2) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.3.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.35.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (4.11.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->keras-rl2) (3.2.0)\n",
            "Installing collected packages: tf-estimator-nightly, keras-rl2\n",
            "Successfully installed keras-rl2-1.0.5 tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Perform Training "
      ],
      "metadata": {
        "id": "bONH2KCDwWp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import EpsGreedyQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define an optimizer\n",
        "Adam = tf.keras.optimizers.Adam\n",
        "\n",
        "# Specify memory\n",
        "memory = SequentialMemory(limit=50000, window_length=1)\n",
        "\n",
        "# Specify the policy to be used\n",
        "policy = EpsGreedyQPolicy()"
      ],
      "metadata": {
        "id": "LnfQP6gRQTTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "If you want to add Dense layers after the Embedding layer, you can choose your own output_dimensions for your Embedding layer (it does not have to follow the action space size), but the final Dense layer must have the same output size as your action space size.\n",
        "\n",
        "In the reshape layer, we take the output from the previous layer and reshape it to a rank 1 tensor (a one-dimensional array). In this notebook, (6,) means a one dimensional array with 6 values. For example, [1, 2, 3, 4, 5, 6] "
      ],
      "metadata": {
        "id": "niwMCwHbky_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modelling\n",
        "model = Sequential()\n",
        "model.add(Embedding(500, 10, input_length=1))\n",
        "model.add(Reshape((10,)))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(action_size, activation='linear'))\n",
        "\n",
        "# Model summary\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jNCfhK_kyI8",
        "outputId": "7f51e13d-1ff5-40df-c045-92d3fdbb2c4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 1, 10)             5000      \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 10)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 50)                550       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 50)                2550      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 50)                2550      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 6)                 306       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10,956\n",
            "Trainable params: 10,956\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the dqn agent\n",
        "dqn = DQNAgent(model=model, nb_actions=action_size, memory=memory, nb_steps_warmup=500, target_model_update=1e-2, policy=policy)\n",
        "\n",
        "# Model compile\n",
        "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
        "\n",
        "# Model fitting\n",
        "dqn.fit(env, nb_steps=1000000, visualize=False, verbose=1, nb_max_episode_steps=99, log_interval=100000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5042wZdk9kX",
        "outputId": "0a0240e5-db76-4f7b-f9ce-809048c74aeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 1000000 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "    47/100000 [..............................] - ETA: 2:07 - reward: -1.1915   "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100000/100000 [==============================] - 897s 9ms/step - reward: -1.2743\n",
            "1091 episodes - episode_reward: -116.780 [-909.000, 15.000] - loss: 4.059 - mae: 25.282 - mean_q: -25.685 - prob: 1.000\n",
            "\n",
            "Interval 2 (100000 steps performed)\n",
            "100000/100000 [==============================] - 929s 9ms/step - reward: -0.5764\n",
            "3787 episodes - episode_reward: -15.222 [-518.000, 15.000] - loss: 1.658 - mae: 12.483 - mean_q: -2.641 - prob: 1.000\n",
            "\n",
            "Interval 3 (200000 steps performed)\n",
            "100000/100000 [==============================] - 950s 9ms/step - reward: 0.1516\n",
            "6763 episodes - episode_reward: 2.240 [-108.000, 15.000] - loss: 0.006 - mae: 7.328 - mean_q: 12.609 - prob: 1.000\n",
            "\n",
            "Interval 4 (300000 steps performed)\n",
            "100000/100000 [==============================] - 946s 9ms/step - reward: 0.1500\n",
            "6788 episodes - episode_reward: 2.211 [-72.000, 15.000] - loss: 0.002 - mae: 7.387 - mean_q: 12.738 - prob: 1.000\n",
            "\n",
            "Interval 5 (400000 steps performed)\n",
            "100000/100000 [==============================] - 948s 9ms/step - reward: 0.1592\n",
            "6775 episodes - episode_reward: 2.350 [-66.000, 15.000] - loss: 0.001 - mae: 7.377 - mean_q: 12.721 - prob: 1.000\n",
            "\n",
            "Interval 6 (500000 steps performed)\n",
            "100000/100000 [==============================] - 954s 10ms/step - reward: 0.1524\n",
            "6800 episodes - episode_reward: 2.242 [-54.000, 15.000] - loss: 0.001 - mae: 7.363 - mean_q: 12.702 - prob: 1.000\n",
            "\n",
            "Interval 7 (600000 steps performed)\n",
            "100000/100000 [==============================] - 953s 10ms/step - reward: 0.1726\n",
            "6817 episodes - episode_reward: 2.529 [-46.000, 15.000] - loss: 0.001 - mae: 7.378 - mean_q: 12.733 - prob: 1.000\n",
            "\n",
            "Interval 8 (700000 steps performed)\n",
            "100000/100000 [==============================] - 959s 10ms/step - reward: 0.1643\n",
            "6784 episodes - episode_reward: 2.422 [-42.000, 15.000] - loss: 0.001 - mae: 7.378 - mean_q: 12.734 - prob: 1.000\n",
            "\n",
            "Interval 9 (800000 steps performed)\n",
            "100000/100000 [==============================] - 963s 10ms/step - reward: 0.1500\n",
            "6797 episodes - episode_reward: 2.206 [-52.000, 15.000] - loss: 0.001 - mae: 7.364 - mean_q: 12.706 - prob: 1.000\n",
            "\n",
            "Interval 10 (900000 steps performed)\n",
            "100000/100000 [==============================] - 962s 10ms/step - reward: 0.1623\n",
            "done, took 9464.467 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f806101c750>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Perform Testing"
      ],
      "metadata": {
        "id": "hKy7wKpowbmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "dqn.test(env, nb_episodes=5, visualize=True, nb_max_episode_steps=99)"
      ],
      "metadata": {
        "id": "SJKXCXVqQvYB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "469255ce-c730-4af9-b95d-eb9821ec72ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 5 episodes ...\n",
            "+---------+\n",
            "|\u001b[42mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "|\u001b[42m_\u001b[0m: | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[42m_\u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[42m_\u001b[0m: : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[42m_\u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[42m_\u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[42m_\u001b[0m: |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35m\u001b[42mB\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "Episode 1: reward: 12.000, steps: 9\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[43m \u001b[0m: |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[42mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[42m_\u001b[0m: |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[42m_\u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[42m_\u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[42m_\u001b[0m: : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| :\u001b[42m_\u001b[0m| : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m:\u001b[42m_\u001b[0m| : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "Episode 2: reward: 8.000, steps: 13\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | :\u001b[43m \u001b[0m: |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[43m \u001b[0m: |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[42mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[42m_\u001b[0m: |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : :\u001b[42m_\u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | :\u001b[42m_\u001b[0m: |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | :\u001b[42m_\u001b[0m:\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :\u001b[35m\u001b[42mG\u001b[0m\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "Episode 3: reward: 10.000, steps: 11\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :\u001b[42mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : :\u001b[42m_\u001b[0m|\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : :\u001b[42m_\u001b[0m|\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[42m_\u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[42m_\u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[42m_\u001b[0m: : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| :\u001b[42m_\u001b[0m| : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m:\u001b[42m_\u001b[0m| : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "Episode 4: reward: 10.000, steps: 11\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | :\u001b[43m \u001b[0m|\n",
            "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : :\u001b[43m \u001b[0m|\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[43m \u001b[0m: : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[43m \u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[43m \u001b[0m| : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[42mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[42m_\u001b[0m| : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[42m_\u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[42m_\u001b[0m: : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[42m_\u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[42m_\u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[42m_\u001b[0m: |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35m\u001b[42mB\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "Episode 5: reward: 4.000, steps: 17\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f805f3ffa90>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Save weights"
      ],
      "metadata": {
        "id": "8OlpDLdDrzJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Saving weights\n",
        "dqn.save_weights('dqn_{}_weights.h5f'.format(\"Taxi-v3\"), overwrite=True)"
      ],
      "metadata": {
        "id": "D4bxB0u3TOgq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}