{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code_RL_Q_Learning_with_Frozen_Lake.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshupandey/Reinforcement-Learning/blob/main/Code_RL_Q_Learning_with_Frozen_Lake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reinforcement Learning "
      ],
      "metadata": {
        "id": "g4Apq6C-z7hP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Temporal Difference and Q Learning"
      ],
      "metadata": {
        "id": "1d4yTqNoz7qv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Objectives\n",
        "***\n",
        "\n",
        "* Q-Learning in Reinforcement Learning"
      ],
      "metadata": {
        "id": "0-E7Nf8y13xz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q-Learning\n",
        "***\n",
        "\n",
        "Q-learning is a model-free reinforcement learning algorithm to learn the value of an action in a particular state. It does not require a model of the environment (hence \"model-free\"), and it can handle problems with stochastic transitions and rewards without requiring adaptations.\n",
        "\n",
        "SARSA is on-policy: the Q function we are using is the Q function we are using in the environment\n",
        "\n",
        "Q-Learning is off-policy: actions are dictated by the epsilon-greedy policy, but we are learning the Q function for the greedy policy. \n",
        "\n",
        "The behaviour policy dictates how we act in the environment.\n",
        "\n",
        "The target policy is the policy we are learning. \n",
        "\n",
        "Q-Learning is a basic form of Reinforcement Learning which uses Q-values (also called action values) to iteratively improve the behavior of the learning agent.\n",
        "\n",
        "**Q-Values or Action-Values:** Q-values are defined for states and actions. Q(S, A) is an estimation of how good is it to take the action A at the state S. This estimation of Q(S, A) will be iteratively computed using the TD- Update rule which we will see in the upcoming sections.\n",
        "\n",
        "**Rewards and Episodes:** An agent over the course of its lifetime starts from a start state, makes a number of transitions from its current state to a next state based on its choice of action and also the environment the agent is interacting in. At every step of transition, the agent from a state takes an action, observes a reward from the environment, and then transits to another state. If at any point of time the agent ends up in one of the terminating states that means there are no further transition possible. This is said to be the completion of an episode.\n",
        "\n",
        "**Temporal Difference or TD-Update:** The Temporal Difference or TD-Update rule can be represented as follows \n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "67a0_znO2Abh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now with all the theory required in hand let us take an example. We will use OpenAI’s gym environment to train our Q-Learning model.\n",
        "\n",
        "Command to Install gym –"
      ],
      "metadata": {
        "id": "YjbO3U9ZMypU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  S : Current State of the agent.\n",
        "  \n",
        "  A : Current Action Picked according to some policy.\n",
        "  \n",
        "  S' : Next State where the agent ends up.\n",
        "  \n",
        "  A' : Next best action to be picked using current Q-value estimation, i.e. pick the action with the maximum Q-value in the next state.\n",
        "  \n",
        "  R : Current Reward observed from the environment in Response of current action.\n",
        "  \n",
        "  $\\gamma$(>0 and <=1) : Discounting Factor for Future Rewards. Future rewards are less valuable than current rewards so they must be discounted. Since Q-value is an estimation of expected rewards from a state, discounting rule applies here as well.\n",
        "  \n",
        "  $\\alpha$ : Step length taken to update the estimation of Q(S, A).\n",
        "\n",
        "A Q-table or matrix is created while performing the Q-learning. The table follows the state and action pair, i.e., [s, a], and initializes the values to zero. After each action, the table is updated, and the q-values are stored within the table.\n",
        "\n",
        "The RL agent uses this Q-table as a reference table to select the best action based on the q-values. "
      ],
      "metadata": {
        "id": "oNMe53WXLzSr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we will use the FrozenLake environment of the gym python library which provides many environments including Atari games and CartPole.\n",
        "\n",
        "FrozenLake environment consists of a 4 by 4 grid representing a surface. The agent always starts from the state 0, [0,0] in the grid, and his goal is to reach the state 16, [4,4] in the grid. On his way, he could find some frozen surfaces or fall in a hole. If he falls, the episode is ended. When the agent reaches the goal, the reward is equal to one. Otherwise, it is equal to 0.\n",
        "\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "Frozen Lake Grid"
      ],
      "metadata": {
        "id": "KlFLyRzmUatG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we import the needed libraries. Numpy for accessing and updating the Q-table and gym to use the FrozenLake environment."
      ],
      "metadata": {
        "id": "SxMd02aAUmga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym"
      ],
      "metadata": {
        "id": "rwRCWL3qNcwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we instantiate our environment and get its sizes."
      ],
      "metadata": {
        "id": "algDK6aoUmbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"FrozenLake-v0\")\n",
        "n_observations = env.observation_space.n\n",
        "n_actions = env.action_space.n"
      ],
      "metadata": {
        "id": "-v9FRSOlP2Bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to create and initialize the Q-table to 0."
      ],
      "metadata": {
        "id": "xXwqtjmiUu9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize the Q-table to 0\n",
        "Q_table = np.zeros((n_observations,n_actions))\n",
        "print(Q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPaXgCtBP6MS",
        "outputId": "bc5d77b9-d609-4291-930f-bead41938e8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the different parameters and hyperparameters "
      ],
      "metadata": {
        "id": "eCIEXH2zU0GV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#number of episode we will run\n",
        "n_episodes = 10000\n",
        "\n",
        "#maximum of iteration per episode\n",
        "max_iter_episode = 100\n",
        "\n",
        "#initialize the exploration probability to 1\n",
        "exploration_proba = 1\n",
        "\n",
        "#exploartion decreasing decay for exponential decreasing\n",
        "exploration_decreasing_decay = 0.001\n",
        "\n",
        "# minimum of exploration proba\n",
        "min_exploration_proba = 0.01\n",
        "\n",
        "#discounted factor\n",
        "gamma = 0.99\n",
        "\n",
        "#learning rate\n",
        "lr = 0.1"
      ],
      "metadata": {
        "id": "0waKT7xeP7q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate the agent training, we will store the total rewards he gets from the environment after each episode in a list that we will use after the training is finished."
      ],
      "metadata": {
        "id": "6OBe-EcmU3IW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_rewards_episode = list()"
      ],
      "metadata": {
        "id": "xinRhQkQP-nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let’s write the main loop where all the process will happen"
      ],
      "metadata": {
        "id": "fC2kCpttU5LJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rewards_per_episode=[]\n",
        "\n",
        "#we iterate over episodes\n",
        "for e in range(n_episodes):\n",
        "    #we initialize the first state of the episode\n",
        "    current_state = env.reset()\n",
        "    done = False\n",
        "    #sum the rewards that the agent gets from the environment\n",
        "    total_episode_reward = 0\n",
        "    \n",
        "    for i in range(max_iter_episode): \n",
        "        # we sample a float from a uniform distribution over 0 and 1\n",
        "        # if the sampled flaot is less than the exploration proba\n",
        "        #     the agent selects arandom action\n",
        "        # else\n",
        "        #     he exploits his knowledge using the bellman equation \n",
        "        \n",
        "        if np.random.uniform(0,1) < exploration_proba:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = np.argmax(Q_table[current_state,:])\n",
        "        \n",
        "        # The environment runs the chosen action and returns\n",
        "        # the next state, a reward and true if the epiosed is ended.\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        \n",
        "        # We update our Q-table using the Q-learning iteration\n",
        "        Q_table[current_state, action] = (1-lr) * Q_table[current_state, action] +lr*(reward + gamma*max(Q_table[next_state,:]))\n",
        "        total_episode_reward = total_episode_reward + reward\n",
        "        # If the episode is finished, we leave the for loop\n",
        "        if done:\n",
        "            break\n",
        "        current_state = next_state\n",
        "    #We update the exploration proba using exponential decay formula \n",
        "    exploration_proba = max(min_exploration_proba, np.exp(-exploration_decreasing_decay*e))\n",
        "    rewards_per_episode.append(total_episode_reward)"
      ],
      "metadata": {
        "id": "oHa05UQwQAcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once our agent is trained, we will test his performance using the rewards per episode list. We will do that by evaluating his performance every 1000 episodes."
      ],
      "metadata": {
        "id": "EkXdlETiT2GJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Mean reward per thousand episodes\")\n",
        "for i in range(10):\n",
        "  print((i+1)*1000, \" : mean espiode reward: \", np.mean(rewards_per_episode[1000*i:1000*(i+1)])) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gM2LNnkxQDJL",
        "outputId": "889e9ec1-c1a4-4791-bb20-3a4b56164e0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean reward per thousand episodes\n",
            "1000  : mean espiode reward:  0.019\n",
            "2000  : mean espiode reward:  0.19\n",
            "3000  : mean espiode reward:  0.443\n",
            "4000  : mean espiode reward:  0.57\n",
            "5000  : mean espiode reward:  0.657\n",
            "6000  : mean espiode reward:  0.667\n",
            "7000  : mean espiode reward:  0.673\n",
            "8000  : mean espiode reward:  0.691\n",
            "9000  : mean espiode reward:  0.694\n",
            "10000  : mean espiode reward:  0.689\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can notice, the performance of the agent is very bad in the beginning but he improved his efficiency through training.\n",
        "\n",
        "Q-learning algorithm is a very efficient way for an agent to learn how the environment works."
      ],
      "metadata": {
        "id": "V5IiS_8SUKd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Thank You !"
      ],
      "metadata": {
        "id": "ugIHxHkaVQAh"
      }
    }
  ]
}